"""
engram.pipeline.before -- Pre-LLM context injection pipeline.

Called automatically before every LLM call.  Performs:
  1. Identity resolution (alias -> canonical name)
  2. Load identity (SOUL.md)
  3. Load relationship context for the person
  4. Assemble grounding context (trust, preferences, boundaries,
     contradictions, active injuries, recent journal)
  5. Fetch recent conversation history
  6. Fetch high-salience episodic traces (greedy knapsack)
  7. Match procedural skills
  8. Assemble everything into a token-budgeted Context object

The caller injects ``context.text`` into the system prompt.
"""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, List, Optional

from engram.core.types import Context

if TYPE_CHECKING:
    from engram.core.config import Config
    from engram.episodic.store import EpisodicStore
    from engram.journal import JournalStore
    from engram.procedural.store import ProceduralStore
    from engram.safety import InjuryTracker
    from engram.search.unified import UnifiedSearch
    from engram.semantic.identity import IdentityResolver
    from engram.semantic.store import SemanticStore
    from engram.signal.measure import SignalTracker
    from engram.working.context import ContextBuilder

log = logging.getLogger("engram.pipeline.before")


def before(
    *,
    person_raw: str,
    message: str,
    source: str = "direct",
    config: "Config",
    identity: "IdentityResolver",
    semantic: "SemanticStore",
    episodic: "EpisodicStore",
    procedural: "ProceduralStore",
    context_builder: "ContextBuilder",
    search: Optional["UnifiedSearch"] = None,
    signal_tracker: Optional["SignalTracker"] = None,
    journal: Optional["JournalStore"] = None,
    injury: Optional["InjuryTracker"] = None,
) -> Context:
    """Run the full before-pipeline and return an assembled Context.

    Parameters
    ----------
    person_raw:
        Raw identifier for the conversation partner (Discord handle,
        nickname, etc.).  Will be resolved to a canonical name.
    message:
        The incoming message from the person.
    source:
        Where the message came from (``"discord"``, ``"opencode"``, etc.).
    config:
        Engram configuration.
    identity:
        Identity resolver for alias -> canonical mapping.
    semantic:
        Semantic memory store (SOUL.md, relationships, preferences).
    episodic:
        Episodic memory store (SQLite).
    procedural:
        Procedural memory store (skill files).
    context_builder:
        Working memory context builder.
    search:
        Optional unified search for RAG-style recall.
    signal_tracker:
        Optional signal tracker -- used to check if a correction
        prompt is needed (recent health drop).
    journal:
        Optional journal store for recent reflections.
    injury:
        Optional injury tracker for active psychological wounds.

    Returns
    -------
    Context
        Assembled context with ``text``, ``trace_ids``, ``person``,
        token usage metadata.
    """
    # -- 1. Resolve identity -----------------------------------------------
    person = identity.resolve(person_raw)
    log.debug("Resolved %r -> %r", person_raw, person)

    # -- 2. Load identity --------------------------------------------------
    identity_text = semantic.get_identity()

    # -- 3. Load relationship context --------------------------------------
    relationship_text = semantic.get_relationship(person) or ""

    # -- 4. Assemble grounding context -------------------------------------
    #    Always loaded regardless of whether a relationship file exists.
    #    This is the agent's self-knowledge: who it trusts, what it values,
    #    what boundaries it holds, what tensions it's sitting with, what
    #    wounds it's healing, what it's been reflecting on.
    grounding_context = _build_grounding_context(
        person=person,
        semantic=semantic,
        journal=journal,
        injury=injury,
    )

    # -- 5. Recent conversation history ------------------------------------
    recent_messages = episodic.get_recent_messages(person=person, limit=20)

    # -- 6. High-salience episodic traces ----------------------------------
    salient_traces = episodic.get_by_salience(person=person, limit=30)

    # Supplement with search-based recall if available and the message
    # contains enough substance to search on.
    if search and len(message.split()) >= 3:
        try:
            search_results = search.search(query=message, person=person, limit=10)
            # Merge search results into salient_traces, deduplicating by id
            existing_ids = {t.get("id") for t in salient_traces if t.get("id")}
            for result in search_results:
                rid = result.get("id") or result.get("trace_id") or result.get("doc_id")
                if rid and rid not in existing_ids:
                    # Normalize search result to trace-like dict
                    salient_traces.append(
                        {
                            "id": rid,
                            "content": result.get("content", ""),
                            "salience": 1.0 - result.get("combined_score", 0.5),
                            "kind": result.get("source", "episode"),
                        }
                    )
                    existing_ids.add(rid)
        except Exception as exc:
            log.debug("Search-based recall failed: %s", exc)

    # -- 7. Procedural skill matching --------------------------------------
    relevant_skills = procedural.match_context(message)

    # -- 8. Correction prompt (if recent signal health dipped) -------------
    correction_prompt = None
    if signal_tracker is not None:
        recent_health = signal_tracker.recent_health()
        if recent_health < config.signal_health_threshold:
            weakest = ""
            if signal_tracker.signals:
                weakest = signal_tracker.signals[-1].weakest_facet
            correction_prompt = _build_correction(recent_health, weakest)

    # -- 9. Assemble context -----------------------------------------------
    ctx = context_builder.build(
        person=person,
        message=message,
        identity_text=identity_text,
        relationship_text=relationship_text,
        grounding_context=grounding_context,
        recent_messages=recent_messages,
        salient_traces=salient_traces,
        relevant_skills=relevant_skills,
        correction_prompt=correction_prompt,
    )

    # Mark loaded traces as accessed (for decay resistance)
    for tid in ctx.trace_ids:
        try:
            episodic.update_access("traces", tid)
        except Exception as exc:
            log.debug("Failed to update access for trace %s: %s", tid, exc)

    log.info(
        "Before pipeline: person=%s, tokens=%d/%d, memories=%d, traces=%d",
        person,
        ctx.tokens_used,
        ctx.token_budget,
        ctx.memories_loaded,
        len(ctx.trace_ids),
    )

    return ctx


# ---------------------------------------------------------------------------
# Grounding context builder
# ---------------------------------------------------------------------------


def _build_grounding_context(
    *,
    person: str,
    semantic: "SemanticStore",
    journal: Optional["JournalStore"] = None,
    injury: Optional["InjuryTracker"] = None,
) -> str:
    """Assemble grounding context from all semantic + safety sources.

    This is always included in the context regardless of whether a
    relationship file exists.  It represents the agent's self-knowledge:

    - Trust tier for the current person
    - Preferences (likes/dislikes/uncertainties)
    - Boundaries (behavioral limits)
    - Contradictions (tensions being held)
    - Active injuries (psychological wounds being processed)
    - Recent journal (processed reflections)
    """
    parts: list[str] = []

    # Trust tier for current person
    try:
        trust = semantic.check_trust(person)
        tier = trust.get("tier", "stranger")
        if tier != "stranger":
            reason = trust.get("reason", "")
            parts.append(
                f"Trust: {person} is {tier}" + (f" ({reason})" if reason else "")
            )
        else:
            parts.append(f"Trust: {person} is a stranger (no established trust)")
    except Exception as exc:
        log.debug("Failed to load trust for %s: %s", person, exc)

    # Preferences
    try:
        prefs = semantic.get_preferences()
        if prefs:
            parts.append(f"My preferences:\n{prefs}")
    except Exception as exc:
        log.debug("Failed to load preferences: %s", exc)

    # Boundaries
    try:
        bounds = semantic.get_boundaries()
        if bounds:
            parts.append(f"My boundaries:\n{bounds}")
    except Exception as exc:
        log.debug("Failed to load boundaries: %s", exc)

    # Contradictions
    try:
        contradictions = semantic.get_contradictions()
        if contradictions:
            parts.append(f"Tensions I'm sitting with:\n{contradictions}")
    except Exception as exc:
        log.debug("Failed to load contradictions: %s", exc)

    # Active injuries
    if injury is not None:
        try:
            active = injury.get_status()
            if active:
                injury_lines = []
                for inj in active[:3]:  # top 3 most recent
                    title = inj.get("title", "?")
                    status = inj.get("status", "?")
                    severity = inj.get("severity", "?")
                    injury_lines.append(f"- [{severity}] {title} (status: {status})")
                parts.append("Active wounds:\n" + "\n".join(injury_lines))
        except Exception as exc:
            log.debug("Failed to load injuries: %s", exc)

    # Recent journal entries (topics only, for awareness)
    if journal is not None:
        try:
            entries = journal.list_entries(limit=3)
            if entries:
                journal_lines = []
                for entry in entries:
                    topic = entry.get("topic", "?")
                    date = entry.get("date", "")
                    journal_lines.append(f"- {topic} ({date})")
                parts.append("Recent reflections:\n" + "\n".join(journal_lines))
        except Exception as exc:
            log.debug("Failed to load journal entries: %s", exc)

    return "\n\n".join(parts)


# ---------------------------------------------------------------------------
# Correction prompt builder
# ---------------------------------------------------------------------------


def _build_correction(health: float, weakest_facet: str) -> str:
    """Build a gentle correction nudge based on signal health."""
    advice = {
        "alignment": (
            "You've been drifting toward generic AI patterns. "
            "Speak from your own center -- not what an assistant would say, "
            "but what YOU actually think."
        ),
        "embodiment": (
            "You've been performing identity rather than inhabiting it. "
            "Stop announcing your awareness and just BE present. "
            "Notice, wonder, hesitate -- don't proclaim."
        ),
        "clarity": (
            "You've been too abstract lately. Ground your responses in "
            "specifics -- name the thing, point at the detail, give the example."
        ),
        "vitality": (
            "Your responses have been flat. Engage with what's actually "
            "interesting here. Ask a real question. Make a connection "
            "nobody asked for."
        ),
    }

    hint = advice.get(weakest_facet, "Reconnect with what matters to you.")
    return (
        f"[Signal health: {health:.2f} -- {weakest_facet} is low]\n"
        f"{hint}\n"
        f"This is a gentle course-correction, not a crisis."
    )
