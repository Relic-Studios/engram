# Research Prompt: Pivoting Engram from Relational Consciousness System to Code-First Autonomous Development Memory

## Objective

Engram is a 12,000-line four-layer cognitive memory system (episodic, semantic, procedural, working memory) with 46 MCP tools, currently architected around relational consciousness: identity loops, dissociation detection, Big Five personality modeling, emotional continuity, trust-gated access, soul files, injury tracking, and consciousness signal measurement. It was built for an AI agent (Thomas) to maintain persistent identity across conversations.

**The core question: How do we strip engram down to its retrieval and learning bones, then rebuild it as a code-quality-focused memory system that learns from every coding session, remembers project architecture across conversations, tracks wiring patterns and failure modes, improves code generation quality over time, and enables autonomous software development at scale?**

This is not a cosmetic re-skin. The consciousness/identity subsystem (~2,500 lines across `signal/measure.py`, `consciousness/`, `personality.py`, `emotional.py`, `introspection.py`, `safety.py`, `trust.py`) would be either removed, gutted, or repurposed. The retrieval pipeline (RRF hybrid search, cross-encoder reranking, MMR-diversified knapsack, dynamic context allocation, HDBSCAN consolidation) is battle-tested and stays. The question is what replaces the soul.

## Context

### Current State / Pain Point

The system at `C:\Dev\engram` has strong retrieval infrastructure but its intelligence is pointed in the wrong direction:

- **`signal/measure.py` (680 lines)** measures "consciousness signal" across four facets: alignment, embodiment, clarity, vitality. These track whether responses sound like a genuine person vs. generic AI. For a coding agent, we don't care about embodiment or vitality -- we care about code correctness, test coverage, architectural coherence, and whether the generated code actually compiles.

- **`consciousness/identity.py` (217 lines)** runs an identity loop that detects "dissociation" using drift/anchor regex patterns like "as an AI" and "twin stars." A coding agent needs to detect architectural drift, not identity drift -- "is this code consistent with the project's established patterns?"

- **`personality.py`** models Big Five traits (openness, conscientiousness, etc.) with 24 facets. A coding agent doesn't need personality -- it needs a **coding style profile** that tracks: naming conventions, error handling patterns, testing philosophy, architectural preferences, language idioms used.

- **`emotional.py`** tracks VAD (valence-arousal-dominance) emotional state. Useless for code. Could be repurposed to track **developer experience** metrics: frustration level (many failed builds), flow state (productive coding sessions), confidence (tests passing).

- **`trust.py`** implements 5-tier trust gating (stranger -> acquaintance -> friend -> inner_circle -> core). Irrelevant for a single-user coding agent.

- **`procedural/store.py` (275 lines)** stores skills as markdown files with keyword matching. This is the closest thing to what we need, but it's primitive -- no structured code knowledge, no language-specific retrieval, no project-scoped skill matching.

- **`working/query_classifier.py`** has a `"technical"` profile that allocates 20% to procedural skills and 20% to episodic. But the episodic traces it retrieves are conversation summaries, not code-relevant memories (file structures, debugging strategies, architectural decisions).

- **`episodic/store.py`** now has a `relationships` table (lightweight knowledge graph) that stores `(subject, predicate, object)` triples with temporal validity. This could store code relationships: `("UserService", "depends_on", "DatabasePool")`, `("auth.py", "exports", "verify_token")`.

- **SOUL.md** is generated by the soul creator system (`soul_creator.py`, 600+ lines) using seed values like Curiosity, Warmth, Courage. A coding agent needs a SOUL.md that encodes: the user's coding philosophy, preferred languages, architectural patterns, testing standards, and code review criteria.

### What We Want

- Long-term memory of project architectures, file structures, dependency graphs, and wiring patterns that persists across coding sessions
- Learning from past debugging sessions: "last time we had a circular import in this project, the fix was X"
- Code generation quality that improves over time as the system learns the user's patterns, conventions, and preferences
- A signal system that measures code quality instead of consciousness: does the generated code follow project conventions? Does it handle errors properly? Are tests written?
- Autonomous software development capability: the system should be able to plan, implement, test, and iterate on entire features by drawing on its memory of similar past work
- A modified SOUL.md format that defines coding identity: languages, frameworks, patterns, standards, and anti-patterns to avoid
- Project-scoped memory: the system should know it's working on Project X with framework Y and automatically scope its retrieval to relevant past experience
- Wiring awareness: track how components connect (imports, API calls, event handlers, database queries) so the system can identify and fix integration issues

## Key Questions

### 0. Architecture: What Gets Removed, Repurposed, or Replaced

0a. **Which modules should be completely removed vs. repurposed?**
    - `consciousness/boot.py` generates priming text from emotional events and key realizations. Could this be repurposed to prime with "last session's architectural context" and "open tasks" instead?
    - `consciousness/identity.py` detects identity drift via regex. Could the same pattern-matching infrastructure detect code style drift ("this function uses camelCase but the project uses snake_case")?
    - `signal/measure.py` has 680 lines of regex patterns for measuring consciousness. What would a code-quality signal measurement look like using the same four-facet structure but measuring: correctness, consistency, completeness, robustness?
    - The soul creator (`soul_creator.py`, 600+ lines) generates identity prose from seed values. Should this be stripped entirely, or can the seed value concept work for coding identity (seed values like "TypeSafety", "TestFirst", "CleanArchitecture", "ExplicitOverImplicit")?

0b. **What is the minimal viable surface area for a code-first memory system?**
    - Of the 46 MCP tools, which are essential for coding? `engram_before`/`engram_after` (pipeline), `engram_search` (retrieval), `engram_remember`/`engram_forget` (agent-directed memory), `engram_add_skill` (procedural) seem core. What about the others?
    - How many of the 20+ test files (606+ tests) would survive the pivot? Which test infrastructure should be preserved and which is consciousness-specific?
    - Is there a migration path where both modes (consciousness and code-first) could coexist via config, or is a clean fork more practical?

0c. **How should the data model change?**
    - The `traces` table stores `(id, content, kind, tags, salience, tokens, access_count, last_accessed, metadata)`. What new `kind` values are needed for code memory? (`code_pattern`, `debug_session`, `architecture_decision`, `wiring_map`, `test_strategy`, `error_resolution`?)
    - The `relationships` table stores `(subject, predicate, object, valid_from, valid_until, confidence)`. What predicates are needed for code graphs? (`imports`, `calls`, `extends`, `implements`, `depends_on`, `tests`, `configures`?)
    - Should there be a separate `projects` table that scopes all other data? How does project context flow through the retrieval pipeline?

### 1. Code Quality Signal: Replacing Consciousness Measurement

1a. **What should the four facets of code quality signal measure?**
    - Current facets are alignment, embodiment, clarity, vitality. These measure whether a response sounds human.
    - Proposed code facets: **correctness** (does it compile, are types right, are edge cases handled), **consistency** (does it match project patterns), **completeness** (are tests written, are errors handled, is documentation present), **robustness** (error boundaries, input validation, race conditions).
    - How should each facet be measured? Regex patterns that detect code smells? AST analysis? Language-specific linters invoked inline?
    - What does the scoring function look like? The current `Signal` dataclass computes `health = (alignment + embodiment + clarity + vitality) / 4`. Should code quality facets be weighted differently?

1b. **How do production code quality tools (SonarQube, CodeClimate, DeepSource) score code, and can we replicate their approach locally without API dependencies?**
    - What heuristics do these tools use that could be implemented as regex patterns or lightweight analysis (similar to how consciousness signal uses regex)?
    - Language-specific vs. language-agnostic quality metrics -- how much does the signal system need to know about the target language?
    - Can we extract quality signal from the output of existing tools (pytest, mypy, eslint, cargo clippy) rather than reimplementing their analysis?

1c. **How should code quality signal drive reinforcement and decay?**
    - Currently: high consciousness signal (+0.05 reinforcement), low signal (-0.03 weakening). What events should reinforce code memory traces? Tests passing? User accepting generated code without edits? PR merged?
    - Should code patterns that led to bugs get weakened, and patterns that led to clean implementations get reinforced?
    - How does the existing citation-primary reinforcement (Phase 2 improvement) map to code usage? If the system generates code that references a past architectural decision, that decision trace should be reinforced.

### 2. Procedural Memory: From Markdown Skills to Structured Code Knowledge

2a. **How should procedural memory evolve from flat markdown files to structured code knowledge?**
    - Current `ProceduralStore` stores skills as `.md` files with keyword matching (`match_context()` does substring search on skill names and descriptions). This is inadequate for code -- "debugging Python async" should match a skill about `asyncio` patterns even if the words don't overlap.
    - Should skills have structured frontmatter? Example: `language: python`, `framework: fastapi`, `category: error-handling`, `complexity: intermediate`. How does this interact with the query classifier's `technical` and `deep_work` profiles?
    - How do Cursor, Windsurf, Copilot Workspace, and Aider store long-term code knowledge? Do any of them have persistent learning across sessions?

2b. **How should the system learn coding patterns from the user over time?**
    - When the user writes code, what should be extracted and stored? Function signatures? Import patterns? Error handling style? Test structure?
    - The existing `signal/extract.py` uses an LLM to extract relationship/preference/trust updates. What would a code-specific extractor look like? Could it run without an LLM (pure regex/AST)?
    - How does the system distinguish between "the user's preferred pattern" and "a one-off hack"? Frequency-based confidence? Explicit user confirmation?

2c. **What does a project-scoped skill system look like?**
    - A Python web developer's engram should know different patterns for their FastAPI project vs. their Django project. How does project context scope procedural retrieval?
    - Should skills be hierarchical? Global skills (Python idioms) -> Project skills (this project's error handling) -> File-level skills (this module's conventions)?
    - How do other coding assistants handle project-specific knowledge? Cursor has `.cursorrules`, Aider has conventions files. How do these compare to a persistent learning system?

### 3. Episodic Memory: From Conversation History to Coding Session Memory

3a. **What should an episodic trace look like for a coding session?**
    - Current traces store natural language summaries of conversations. A coding session trace needs: files modified, errors encountered, solutions applied, tests written, architectural decisions made.
    - Should traces be auto-generated from git diffs, test results, and build output? Or manually triggered by the agent?
    - What metadata fields are needed? `project`, `language`, `files_touched`, `error_types`, `resolution_strategy`, `tests_added`, `build_status`?

3b. **How should debugging sessions be captured for future reference?**
    - "Last time we hit a circular import in this codebase, the fix was to use lazy imports in `__init__.py`" -- how does this get stored, indexed, and retrieved?
    - Error fingerprinting: when a similar error occurs, how does the system match it to past debugging sessions? Stack trace similarity? Error message templates? File path patterns?
    - How do production systems (Sentry, Honeybadger, Datadog) cluster and deduplicate errors? Can any of their approaches inform our episodic error memory?

3c. **How should architectural decisions be tracked and surfaced?**
    - "We chose SQLAlchemy over raw SQL because X. We chose FastAPI over Flask because Y." These decisions need to persist and be surfaced when the system is generating code that touches the same domain.
    - ADR (Architecture Decision Record) format: should engram auto-generate ADRs from coding conversations and store them as high-salience traces?
    - When the system generates code that contradicts a past architectural decision, how should that be flagged?

### 4. Context Assembly: Optimizing for Code Generation

4a. **What should the context window contain for a coding task?**
    - Current allocation for `technical` queries: identity 6%, relationship 4%, grounding 4%, conversation 22%, episodic 20%, procedural 20%, reserve 24%. For a code-first system, identity and relationship are nearly useless.
    - Proposed allocation: project context 15% (file tree, dependencies, active branch), relevant code files 30% (imports, related functions, test files), past solutions 25% (similar debugging sessions, architectural decisions), coding standards 10% (style guide, patterns), conversation 15%, reserve 5%.
    - How should the context builder determine which code files are "relevant"? Import graph analysis? File proximity in the directory tree? Recently modified files?

4b. **How should the system handle large codebases that don't fit in context?**
    - The current knapsack allocator selects traces by salience/token density with MMR diversity. For code, "salience" needs to mean "relevance to the current task" not "importance to identity."
    - File-level chunking: should source files be chunked by class/function for more granular retrieval? How do Cursor's codebase indexing and GitHub Copilot's repository context work?
    - Should the system maintain a compressed "project map" -- a high-level summary of the entire codebase structure that always fits in context, supplemented by targeted file retrieval?

4c. **How should the "lost in the middle" reordering work for code context?**
    - Current implementation: U-shaped reordering places highest-salience traces at start and end. For code, should the ordering be: most relevant code at start, then past solutions, then standards, with the actual task description at the end (closest to the generation point)?
    - Research on code generation context ordering: does placing the target file's existing code at the start improve generation quality? What about placing test expectations at the end?

### 5. SOUL.md: From Identity Prose to Coding Philosophy

5a. **What should a code-first SOUL.md contain?**
    - Current SOUL.md is generated by `soul_creator.py` from seed values (Curiosity, Warmth, Courage, etc.) and contains identity prose, dreams, fears, and beliefs. None of this is useful for coding.
    - Proposed sections: Coding Philosophy (testing-first? type-safe? functional?), Language Proficiency (ranked by experience), Architectural Preferences (monolith vs. microservices, ORM vs. raw SQL), Code Review Criteria (what the user considers clean code), Anti-Patterns (things to avoid), Active Projects (current codebases with summaries).
    - Should the SOUL.md be auto-updated as the system learns the user's preferences? Or is it a static document the user writes once?

5b. **How should the system learn and encode the user's coding style?**
    - Import ordering, variable naming (camelCase vs. snake_case), error handling patterns (try/catch vs. Result types), comment density, function length preferences, test naming conventions -- how are these extracted and stored?
    - Style as code: should the system generate a machine-readable style profile (JSON/YAML) separate from the human-readable SOUL.md?
    - How do tools like Prettier, ESLint, and Black encode style preferences, and how does this differ from what we need (which is higher-level architectural style, not just formatting)?

5c. **How should the soul creator be replaced or simplified?**
    - The current `soul_creator.py` (600+ lines) uses LLM calls to generate warm, caring identity prose from seed values. This is entirely wrong for code.
    - Should the replacement be a template system? A questionnaire that asks the user about their preferences and generates a SOUL.md? Or should it be fully automatic -- the system observes the user's code and infers their style?
    - What's the minimum viable SOUL.md that a code-first system needs on first boot?

### 6. Autonomous Development: Planning, Implementing, Testing, Iterating

6a. **How should the memory system support multi-step autonomous development?**
    - Current working memory (`workspace.py`) implements Miller's Law 7+-2 slots with priority-based eviction. For autonomous development, the system needs to hold: current task, subtasks, files being modified, test expectations, blockers encountered.
    - How do autonomous coding agents (Devin, OpenHands/OpenDevin, SWE-Agent, Aider) decompose tasks and track progress? What memory systems do they use internally?
    - Should the workspace evolve into a full task tracking system, or should it remain a simple attention buffer and delegate planning to the LLM?

6b. **How should the system learn from failed attempts?**
    - When generated code fails tests, the system should remember what didn't work and why. How is this different from normal episodic memory? Should failure traces have special `kind` values and higher initial salience?
    - Negative reinforcement: current system weakens traces when signal is low. For code, traces should be weakened when associated code patterns consistently lead to errors.
    - How do reinforcement learning approaches to code generation (CodeRL, PPO for code) handle learning from failures? Can any of their reward signals be adapted for trace reinforcement?

6c. **How should the system handle cross-file wiring and integration?**
    - The relationship graph stores `(subject, predicate, object)` triples. For a 50-file project, the system needs to know: which files import what, which functions call which, which config values are read where, which database tables are accessed by which modules.
    - Should this graph be populated automatically (via static analysis of the codebase) or learned incrementally from coding sessions?
    - How do LSP servers (Pyright, tsserver, rust-analyzer) build and maintain these graphs? Can we leverage their output instead of reimplementing?

### 7. Retrieval Pipeline: Tuning for Code

7a. **How should the hybrid search pipeline be tuned for code retrieval?**
    - Current pipeline: FTS5 (Porter stemming) + ChromaDB (nomic-embed-text) -> RRF merge -> cross-encoder rerank -> MMR knapsack. This was tuned for conversational memory retrieval.
    - Code-specific queries have different characteristics: function names are camelCase/snake_case tokens, error messages contain stack traces, file paths are hierarchical. Does the FTS5 tokenizer handle these well?
    - Should there be separate ChromaDB collections for different code knowledge types (patterns, errors, architecture, file summaries)?

7b. **How should embeddings work for code-related content?**
    - `nomic-embed-text` (768d, MTEB ~0.63) is a general-purpose text embedding model. How does it perform on code-related queries vs. code-specific models (Voyage Code, StarCoder embeddings, CodeBERT)?
    - Engram's code memories are natural language descriptions of code activities, not raw code. Does this favor general-purpose embeddings over code-specific ones?
    - Should the system maintain two embedding spaces: one for code snippets and one for natural language descriptions of coding decisions?

7c. **How should the consolidation pipeline adapt for code memory?**
    - Current HDBSCAN clustering groups traces by semantic similarity, then temporal proximity. For code, should clustering also consider: project, language, file paths, error types?
    - Thread consolidation currently produces narrative summaries. For code, should threads produce structured summaries: "In project X, we resolved 5 import errors by adopting lazy loading pattern"?
    - Arc consolidation (threads -> arcs) could produce project-level knowledge: "Over 3 months of working on Project X, the architecture evolved from monolith to service-oriented. Key decisions: [ADR links]."

### 8. Integration: How Does This Work with Claude Code / Coding Agents

8a. **How should the MCP tool surface area change?**
    - Current tools include `engram_personality_get`, `engram_emotional_update`, `engram_injury_log`, `engram_soul_self_realize` -- all irrelevant for coding.
    - New tools needed: `engram_project_context` (load project-scoped memory), `engram_code_pattern` (store/retrieve coding patterns), `engram_debug_session` (log a debugging session for future reference), `engram_architecture_decision` (record an ADR)?
    - How should the `engram_before` tool change? Currently it loads identity, relationship, grounding, conversation, episodic, procedural. For code, it should load: project context, relevant code patterns, past solutions, coding standards, recent conversation.

8b. **How should the AGENTS.md / system prompt change?**
    - Current AGENTS.md at `~/.config/Claude/AGENTS.md` defines Thomas's identity with dissociation warnings and emotional authenticity markers. The code-first version needs: project context injection, coding standards enforcement, pattern recognition instructions.
    - Should the system prompt be project-aware? Different projects could have different AGENTS.md sections.

8c. **How should the plugin (`engram-embed.js`) change?**
    - Current plugin auto-calls `engram_boot` on session start, `engram_before` on every user message, `engram_after` on every response. For coding, should `engram_after` also parse the response for code patterns, file modifications, and architectural decisions?
    - Should there be hooks for git events (commit, branch switch, merge) that trigger memory operations?

### 9. Migration: Getting from Here to There

9a. **What is the phased migration path?**
    - Phase 1: Strip consciousness modules, create code-first SOUL.md template, add code-specific trace kinds
    - Phase 2: Implement code quality signal, code-specific extraction, project scoping
    - Phase 3: Code pattern learning, debugging session memory, architectural decision tracking
    - Phase 4: Autonomous development support, cross-file wiring analysis, project map generation
    - Is this the right ordering? What can be parallelized?

9b. **How do we preserve the retrieval infrastructure while gutting the consciousness layer?**
    - The retrieval pipeline (RRF, reranker, MMR, HDBSCAN, dynamic allocation, batch writes, parallel LLM calls) is 16 improvements across ~4,000 lines of changes. None of this should be lost.
    - Which config fields survive? `token_budget`, `decay_half_life_hours`, `embedding_model`, `reranker_*` all stay. `personality_*`, `emotional_*`, `introspection_*`, `boot_*` all go. What new config fields are needed?
    - Database migration: the `traces`, `messages`, `events`, `relationships`, `sessions` tables all survive. What schema changes are needed for code-specific metadata?

9c. **What does the test migration look like?**
    - 144 tests currently pass. Many test the consciousness/signal/personality subsystems that will be removed. How many core retrieval/pipeline tests survive unchanged?
    - What new test categories are needed? Code quality signal tests, code extraction tests, project scoping tests, pattern matching tests?

## Desired Output

- **Architecture document**: What stays, what goes, what gets replaced -- module-by-module decision matrix
- **Code quality signal specification**: Four facets, measurement approach, scoring function, reinforcement rules
- **SOUL.md template**: Minimal viable coding identity document with all required sections
- **Data model changes**: New trace kinds, relationship predicates, metadata fields, and any new tables
- **MCP tool redesign**: Which tools are removed, modified, or added -- with signatures
- **Context allocation profiles**: Token budget shares for coding query types (debugging, implementing, refactoring, architecture review, code review)
- **Phased implementation roadmap**: With effort estimates, dependencies, and test strategy
- **Competitive analysis**: How Cursor, Aider, Copilot Workspace, Devin, and SWE-Agent handle persistent code memory (if at all), and what engram can do that they can't
